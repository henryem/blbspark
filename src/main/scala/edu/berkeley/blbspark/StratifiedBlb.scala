package edu.berkeley.blbspark

import dist.BernoulliDistribution
import spark.{Logging, RDD}
import spark.SparkContext._
import java.util.Random
import cern.jet.random.Poisson
import cern.jet.random.engine.DRand

object StratifiedBlb extends Logging {

  //FIXME: Document
  //FIXME: Remove type parameters
  abstract class RddOperator[D,I,R]
  case class ArbitraryRddOperator[D,I,R](func: (RDD[D] => R)) extends RddOperator[D,I,R]
  //FIXME: Document
  case class CommutativeMonoidRddOperator[D,I,R](
      zero: I,
      projectAndAdd: ((I, D )=> I),
      add: ((I, I) => I),
      mapToResult: (I => R)
    ) extends RddOperator[D,I,R]

  /**
   * Perform the stratified bootstrap on @originalSample, which is assumed
   * be a stratified sample from some original (larger) dataset, using the
   * strata generated by @stratifier.  The goal of the bootstrap is to estimate
   * some property @xi of the distribution of @theta under repeated sampling
   * from the original dataset.  The bootstrap attempts to do this without
   * actually having access to the original dataset.
   *
   * FIXME: Finish documentation.
   * FIXME: Figure out a way to make the interface less intimidating.
   *
   * @param originalSample: A stratified sample from some original (larger)
   *                    dataset.
   * @param stratifier
   * @param theta
   * @param xi
   * @param average
   * @param alpha
   * @param s
   * @param r
   * @param k
   * @param numSplits
   * @param seed
   * @tparam D
   * @tparam G: The type of object on which @originalSample is stratified.
   *          NOTE: It is very important that objects of this type implement
   *          a hashCode() that is consistent across JVM instances, meaning
   *          that it remains the same after a G is serialized and then
   *          deserialized in another JVM.  In particular, the default
   *          Object.hashCode() and the hashCode() of Java enums do not have
   *          this consistency guarantee.
   * @tparam I
   * @tparam R
   * @tparam S
   * @return
   */
  def stratifiedBlb[D: ClassManifest, G: ClassManifest, I: ClassManifest, R, S: ClassManifest](
      originalSample: RDD[WeightedItem[D]],
      stratifier: (WeightedItem[D] => G),
      theta: RddOperator[WeightedItem[D],I,R],
      xi: (Seq[R] => S),
      average: (Seq[S] => S),
      alpha: Double,
      s: Int,
      r: Int,
      k: Int,
      numSplits: Int,
      seed: Int):
      S = {
    require(0.0 <= alpha && alpha <= 1.0)
    if (alpha < 0.5) {
      logWarning("alpha=%f is too small.  This may cause error to be overestimated.".format(alpha))
    }
    require(s > 0)
    require(r > 0)
    require(k > 0)
    if (k < 1000) {
      logWarning("k=%d is too small.  This may cause error to be overestimated.".format(k))
    }

    //TODO: Handle various corner cases (empty data, etc).
    //TODO: Expand API to handle finite sample correction(?)
    //TODO: Expand API to handle prefiltering of @originalSample.
    val dataByGroupSize = splitDataByGroupSize(originalSample, stratifier, k)
    val subsampleSize: Double = math.floor(dataByGroupSize.numLargeGroups * math.pow(k, alpha))
    val totalLargeGroupCount = dataByGroupSize.numLargeGroups * k
    val largeGroupResamples: Seq[Seq[RDD[WeightedItem[D]]]] = if (subsampleSize > 0) {
      val largeGroupDataCached = dataByGroupSize.largeGroupData.cache()
      val subsamplingProportion = subsampleSize.toFloat / totalLargeGroupCount
      require(s*subsamplingProportion <= 1.0, "total size of the subsamples (s*N^alpha) exceeds the sample size") //TODO: Probably don't actually want to throw an exception here.
      val largeGroupSubsamples: Seq[RDD[WeightedItem[D]]] = subsample(largeGroupDataCached, subsamplingProportion, s, seed)
      val resamplingRate = 1.0 / subsamplingProportion
      createResamples(largeGroupSubsamples, resamplingRate, r, seed)
    } else {
      // Shortcut: If there are no large groups, we don't need to do much.
      (0 until s).map(subsampleIdx => (0 until r).map(resampleIdx => originalSample.context.parallelize[WeightedItem[D]](Nil, 1)))
    }
    val thetaValues: Seq[Seq[R]] = computeEstimates(theta, largeGroupResamples, dataByGroupSize.smallGroupData)
    val xiValues: Seq[S] = thetaValues.map(xi)
    average(xiValues)
  }

  /**
   * As stratifiedBlb, but uses a different algorithm, the ordinary bootstrap.
   * This will be slower for large datasets.
   */
  def stratifiedBootstrap[D: ClassManifest, G: ClassManifest, I: ClassManifest, R, S: ClassManifest](
    originalSample: RDD[WeightedItem[D]],
    stratifier: (WeightedItem[D] => G),
    theta: RddOperator[WeightedItem[D],I,R],
    xi: (Seq[R] => S),
    r: Int,
    k: Int,
    numSplits: Int,
    seed: Int):
    S = {
    val dataByGroupSize = splitDataByGroupSize(originalSample, stratifier, k)
    val totalLargeGroupCount = dataByGroupSize.numLargeGroups * k
    val largeGroupResamples: Seq[RDD[WeightedItem[D]]] = if (totalLargeGroupCount > 0) {
      //TODO: Check whether this caching step is necessary.
      val largeGroupDataCached = dataByGroupSize.largeGroupData.cache()
      (0 until r).map({resampleIdx => resample(largeGroupDataCached, 1.0, seed + numSplits*resampleIdx)})
    } else {
      // Shortcut: If there are no large groups, we don't need to do much.
      (0 until r).map({resampleIdx => originalSample.context.parallelize[WeightedItem[D]](Nil, 1)})
    }
    val thetaValues: Seq[R] = computeEstimatesForResamples(theta, largeGroupResamples, dataByGroupSize.smallGroupData)
    xi(thetaValues)
  }

  private def subsample[D: ClassManifest](data: RDD[D], subsampleProportion: Double, numSubsamples: Int, seed: Int): Seq[RDD[D]] = {
    val approximatelyPartitionedDataCached = data.mapPartitionsWithSplit(
      (splitIdx: Int, partition: Iterator[D]) => {
        val splitSeed = seed + splitIdx
        val random = new Random(splitSeed)
        val bernoulli = new BernoulliDistribution(math.min(subsampleProportion * numSubsamples, 1.0), splitSeed)
        partition.filter({d => bernoulli.sample()}).map({d => (d, random.nextInt(numSubsamples))})
      })
      .cache()
    (0 until numSubsamples).map({subsampleIdx => approximatelyPartitionedDataCached.filter(_._2 == subsampleIdx).map(_._1)})
  }

  private def createResamples[D](subsamples: Seq[RDD[WeightedItem[D]]], samplingRate: Double, numResamples: Int, seed: Int): Seq[Seq[RDD[WeightedItem[D]]]] = {
    subsamples.zipWithIndex.map({
      case (subsample: RDD[WeightedItem[D]], subsampleIdx: Int) =>
        //TODO: May be good to cache @subsample here.
        val numSplits = subsample.splits.length
        (0 until numResamples).map({resampleIdx => resample(subsample, samplingRate, seed + numSplits*(numResamples*subsampleIdx + resampleIdx))})
    })
  }

  //TODO: Document.  The seed space [seed, seed + subsample.numSplits) will be used.
  private def resample[D](subsample: RDD[WeightedItem[D]], samplingRate: Double, seed: Int): RDD[WeightedItem[D]] = {
    subsample.mapPartitionsWithSplit(
      (splitIdx: Int, partition: Iterator[WeightedItem[D]]) => {
        val poissonDistribution = new Poisson(samplingRate, new DRand(seed + splitIdx))
        partition.map({d => WeightedItem(d.item, d.weight * poissonDistribution.nextInt())})
      })
  }

  private def computeEstimates[D,I: ClassManifest,R](
      theta: RddOperator[D,I,R],
      largeGroupResamples: Seq[Seq[RDD[D]]],
      smallGroupData: RDD[D]):
      Seq[Seq[R]] = {
    require(largeGroupResamples.size > 0)
    theta match {
      case op: ArbitraryRddOperator[D,I,R] => {
        val glommedResamples: Seq[Seq[RDD[D]]] = largeGroupResamples.map(_.map(_.union(smallGroupData)))
        glommedResamples.map(_.map(op.func))
      }
      case op: CommutativeMonoidRddOperator[D,I,R] => {
        // If the operator @theta has additional structure, we can be more
        // efficient.  We can compute the result on the small-group data once
        // and then add it to each of the results from the large-group data
        // resamples.
        val smallDataIntermediateResult = smallGroupData.aggregate(op.zero)(op.projectAndAdd, op.add)
        val largeDataIntermediateResults = largeGroupResamples.map(_.map(_.aggregate(op.zero)(op.projectAndAdd, op.add)))
        largeDataIntermediateResults.map(_.map({intermediate => op.mapToResult(op.add(smallDataIntermediateResult, intermediate))}))
      }
    }
  }

  //FIXME: Move this and other bootstrap stuff to another class.
  private def computeEstimatesForResamples[D,I: ClassManifest,R](
    theta: RddOperator[D,I,R],
    largeGroupResamples: Seq[RDD[D]],
    smallGroupData: RDD[D]):
    Seq[R] = {
    require(largeGroupResamples.size > 0)
    theta match {
      case op: ArbitraryRddOperator[D,I,R] => {
        val glommedResamples: Seq[RDD[D]] = largeGroupResamples.map(_.union(smallGroupData))
        glommedResamples.map(op.func)
      }
      case op: CommutativeMonoidRddOperator[D,I,R] => {
        // If the operator @theta has additional structure, we can be more
        // efficient.  We can compute the result on the small-group data once
        // and then add it to each of the results from the large-group data
        // resamples.
        val smallDataIntermediateResult = smallGroupData.aggregate(op.zero)(op.projectAndAdd, op.add)
        val largeDataIntermediateResults = largeGroupResamples.map(_.aggregate(op.zero)(op.projectAndAdd, op.add))
        largeDataIntermediateResults.map({intermediate => op.mapToResult(op.add(smallDataIntermediateResult, intermediate))})
      }
    }
  }

  private case class DataByGroupSize[T, G](
    smallGroupData: RDD[T],
    largeGroupData: RDD[T],
    numLargeGroups: Long)

  private def splitDataByGroupSize[T: ClassManifest,G: ClassManifest](data: RDD[T], grouper: (T => G), k: Int): DataByGroupSize[T, G] = {
    val groupedData: RDD[(G, T)] = data.map({t: T => (grouper(t), t)})
    val dataGroupCountsCached: RDD[(G, Int)] = groupedData.combineByKey({t: T => 1}, {(c: Int, t: T) => c + 1}, {(c1: Int, c2: Int) => c1 + c2})
      .cache()
    val numLargeGroups = dataGroupCountsCached.filter(_._2 >= k).count
    val dataWithCountsCached = groupedData.join(dataGroupCountsCached)
      .map(_._2)
      .cache()
    val smallGroupData = dataWithCountsCached.filter(_._2 < k).map(_._1)
    val largeGroupData = dataWithCountsCached.filter(_._2 >= k).map(_._1)
    DataByGroupSize(smallGroupData, largeGroupData, numLargeGroups)
  }
}
